{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "254a2a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import datasets, layers, models, losses, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af18d937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 345 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "training_set = train_datagen.flow_from_directory('BreastsData/training_set',\n",
    "                                                 target_size = (224, 224),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dea8030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 71 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_set = test_datagen.flow_from_directory('BreastsData/test_set',\n",
    "                                            target_size = (224, 224),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22f844b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The input must have 3 channels; got `input_shape=(224, 224, 1)`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-092f4a2f14f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# base_model = tf.keras.applications.ResNet152(weights = 'imagenet', include_top = False, input_shape = (224,224,3))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m base_model = tf.keras.applications.VGG16(\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0minclude_top\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"imagenet\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\razva\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\applications\\vgg16.py\u001b[0m in \u001b[0;36mVGG16\u001b[1;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[0;32m    124\u001b[0m                      ' as true, `classes` should be 1000')\n\u001b[0;32m    125\u001b[0m   \u001b[1;31m# Determine proper input shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m   input_shape = imagenet_utils.obtain_input_shape(\n\u001b[0m\u001b[0;32m    127\u001b[0m       \u001b[0minput_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m       \u001b[0mdefault_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\razva\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\applications\\imagenet_utils.py\u001b[0m in \u001b[0;36mobtain_input_shape\u001b[1;34m(input_shape, default_size, min_size, data_format, require_flatten, weights)\u001b[0m\n\u001b[0;32m    363\u001b[0m           \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'`input_shape` must be a tuple of three integers.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'imagenet'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m           raise ValueError('The input must have 3 channels; got '\n\u001b[0m\u001b[0;32m    366\u001b[0m                            '`input_shape=' + str(input_shape) + '`')\n\u001b[0;32m    367\u001b[0m         if ((input_shape[0] is not None and input_shape[0] < min_size) or\n",
      "\u001b[1;31mValueError\u001b[0m: The input must have 3 channels; got `input_shape=(224, 224, 1)`"
     ]
    }
   ],
   "source": [
    "# base_model = tf.keras.applications.ResNet152(weights = 'imagenet', include_top = False, input_shape = (224,224,3))\n",
    "base_model = tf.keras.applications.VGG16(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(224,224,3),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7147551",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "  layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d838fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = layers.Flatten()(base_model.output)\n",
    "x = layers.Dense(1000, activation='relu')(x)\n",
    "predictions = layers.Dense(3, activation = 'softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0884970f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs = base_model.input, outputs = predictions)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy',tf.keras.metrics.Precision()])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01701861",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x = training_set, validation_data = test_set, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32bbdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(15,15))\n",
    "\n",
    "axs[0].plot(history.history['loss'])\n",
    "axs[0].plot(history.history['val_loss'])\n",
    "axs[0].title.set_text('Training Loss vs Validation Loss')\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[0].legend(['Train','Val'])\n",
    "\n",
    "axs[1].plot(history.history['accuracy'])\n",
    "axs[1].plot(history.history['val_accuracy'])\n",
    "axs[1].title.set_text('Training Accuracy vs Validation Accuracy')\n",
    "axs[1].set_xlabel('Epochs')\n",
    "axs[1].set_ylabel('Accuracy')\n",
    "axs[1].legend(['Train', 'Val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b40ff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "test_image = image.load_img('OnePrediction/cancer/5.jpg', target_size = (224, 224))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = model.predict(test_image)\n",
    "training_set.class_indices\n",
    "\n",
    "\n",
    "the_result = -1\n",
    "value=float('-inf')\n",
    "\n",
    "for i in range(len(result[0])):\n",
    "    if result[0][i]-value>0.00001: \n",
    "        the_result = i\n",
    "        value = result[0][i]\n",
    "\n",
    "if the_result == 0:\n",
    "    prediction = 'benign'\n",
    "elif the_result:\n",
    "    prediction = 'cancer'\n",
    "else: \n",
    "    prediction = 'normal'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a79e981",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
