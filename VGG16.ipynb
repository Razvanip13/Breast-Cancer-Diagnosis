{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93b7b123",
   "metadata": {},
   "source": [
    "# VGG Breast Cancer Diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1aca8a",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4d50a378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "D:\\Users\\razva\\anaconda3\\envs\\tf\\lib\\site-packages\\PIL\\Image.py\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "print(tf.__version__)\n",
    "\n",
    "from PIL import Image\n",
    "print(Image.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0bd6471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense,Activation,Dropout,Flatten, Conv2D, MaxPooling2D,MaxPool2D\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9b4380e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from PIL import Image\n",
    "sys.modules['Image'] = Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f533dd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Users\\razva\\anaconda3\\envs\\tf\\lib\\site-packages\\PIL\\Image.py\n"
     ]
    }
   ],
   "source": [
    "import Image\n",
    "print(Image.__file__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7895c105",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3c2ea350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 345 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "training_set = train_datagen.flow_from_directory('BreastsData/training_set',\n",
    "                                                 target_size = (224, 224),\n",
    "                                                 batch_size = 32,\n",
    "                                                 color_mode='grayscale',\n",
    "                                                 class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1391d495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 71 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_set = test_datagen.flow_from_directory('BreastsData/test_set',\n",
    "                                            target_size = (224, 224),\n",
    "                                            batch_size = 32,\n",
    "                                            color_mode='grayscale',\n",
    "                                            class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fc35e3",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "595c93bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_54 (Conv2D)           (None, 224, 224, 64)      640       \n",
      "_________________________________________________________________\n",
      "conv2d_55 (Conv2D)           (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_56 (Conv2D)           (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "conv2d_57 (Conv2D)           (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_58 (Conv2D)           (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_59 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_60 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_61 (Conv2D)           (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_62 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_63 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_64 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_65 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_66 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 3)                 12291     \n",
      "=================================================================\n",
      "Total params: 134,271,683\n",
      "Trainable params: 134,271,683\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn = Sequential()\n",
    "\n",
    "# First double set\n",
    "cnn.add(Conv2D(filters=64, kernel_size=3, activation='relu',padding=\"same\", input_shape=[224,224,1]))\n",
    "cnn.add(Conv2D(filters=64, kernel_size=3, activation='relu',padding=\"same\"))\n",
    "cnn.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "# Second double set\n",
    "cnn.add(Conv2D(filters=128, kernel_size=3, activation='relu',padding=\"same\"))\n",
    "cnn.add(Conv2D(filters=128, kernel_size=3, activation='relu',padding=\"same\"))\n",
    "cnn.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "# First triple set\n",
    "cnn.add(Conv2D(filters=256, kernel_size=3, activation='relu',padding=\"same\"))\n",
    "cnn.add(Conv2D(filters=256, kernel_size=3, activation='relu',padding=\"same\"))\n",
    "cnn.add(Conv2D(filters=256, kernel_size=3, activation='relu',padding=\"same\"))\n",
    "cnn.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "# Second triple set\n",
    "cnn.add(Conv2D(filters=512, kernel_size=3, activation='relu',padding=\"same\"))\n",
    "cnn.add(Conv2D(filters=512, kernel_size=3, activation='relu',padding=\"same\"))\n",
    "cnn.add(Conv2D(filters=512, kernel_size=3, activation='relu',padding=\"same\"))\n",
    "cnn.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "# Third triple set\n",
    "cnn.add(Conv2D(filters=512, kernel_size=3, activation='relu',padding=\"same\"))\n",
    "cnn.add(Conv2D(filters=512, kernel_size=3, activation='relu',padding=\"same\"))\n",
    "cnn.add(Conv2D(filters=512, kernel_size=3, activation='relu',padding=\"same\"))\n",
    "cnn.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "# Flattening\n",
    "cnn.add(Flatten())\n",
    "\n",
    "#Double dense layers\n",
    "cnn.add(Dense(units=4096, activation='relu'))\n",
    "cnn.add(Dropout(0.4))\n",
    "cnn.add(Dense(units=4096, activation='relu'))\n",
    "cnn.add(Dropout(0.4))\n",
    "\n",
    "#Output layer\n",
    "cnn.add(Dense(units=3, activation='softmax'))\n",
    "\n",
    "\n",
    "cnn.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6dc5f186",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy',tf.keras.metrics.Precision()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f9cad75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "11/11 [==============================] - 156s 14s/step - loss: 1.1405 - accuracy: 0.3362 - precision_4: 0.4688 - val_loss: 1.0129 - val_accuracy: 0.5634 - val_precision_4: 0.0000e+00\n",
      "Epoch 2/15\n",
      "11/11 [==============================] - 158s 14s/step - loss: 1.0692 - accuracy: 0.4638 - precision_4: 0.3333 - val_loss: 1.0112 - val_accuracy: 0.5634 - val_precision_4: 0.0000e+00\n",
      "Epoch 3/15\n",
      "11/11 [==============================] - 153s 14s/step - loss: 1.0710 - accuracy: 0.4638 - precision_4: 0.2308 - val_loss: 1.0295 - val_accuracy: 0.5634 - val_precision_4: 0.0000e+00\n",
      "Epoch 4/15\n",
      "11/11 [==============================] - 152s 14s/step - loss: 1.0654 - accuracy: 0.4638 - precision_4: 0.6667 - val_loss: 1.0064 - val_accuracy: 0.5634 - val_precision_4: 0.0000e+00\n",
      "Epoch 5/15\n",
      "11/11 [==============================] - 155s 14s/step - loss: 1.0629 - accuracy: 0.4638 - precision_4: 0.5455 - val_loss: 1.0044 - val_accuracy: 0.5634 - val_precision_4: 0.0000e+00\n",
      "Epoch 6/15\n",
      "11/11 [==============================] - 167s 15s/step - loss: 1.0702 - accuracy: 0.4638 - precision_4: 0.0000e+00 - val_loss: 1.0294 - val_accuracy: 0.5634 - val_precision_4: 0.0000e+00\n",
      "Epoch 7/15\n",
      "11/11 [==============================] - 169s 15s/step - loss: 1.0648 - accuracy: 0.4638 - precision_4: 0.0000e+00 - val_loss: 1.0107 - val_accuracy: 0.5634 - val_precision_4: 0.0000e+00\n",
      "Epoch 8/15\n",
      "11/11 [==============================] - 163s 15s/step - loss: 1.0675 - accuracy: 0.4638 - precision_4: 0.4038 - val_loss: 1.0013 - val_accuracy: 0.5634 - val_precision_4: 0.0000e+00\n",
      "Epoch 9/15\n",
      "11/11 [==============================] - 158s 14s/step - loss: 1.0632 - accuracy: 0.4638 - precision_4: 0.3333 - val_loss: 1.0116 - val_accuracy: 0.5634 - val_precision_4: 0.0000e+00\n",
      "Epoch 10/15\n",
      "11/11 [==============================] - 152s 14s/step - loss: 1.0652 - accuracy: 0.4638 - precision_4: 1.0000 - val_loss: 1.0047 - val_accuracy: 0.5634 - val_precision_4: 0.0000e+00\n",
      "Epoch 11/15\n",
      "11/11 [==============================] - 151s 14s/step - loss: 1.0676 - accuracy: 0.4638 - precision_4: 0.2941 - val_loss: 1.0076 - val_accuracy: 0.5634 - val_precision_4: 0.0000e+00\n",
      "Epoch 12/15\n",
      "11/11 [==============================] - 160s 15s/step - loss: 1.0659 - accuracy: 0.4638 - precision_4: 0.0000e+00 - val_loss: 1.0216 - val_accuracy: 0.5634 - val_precision_4: 0.0000e+00\n",
      "Epoch 13/15\n",
      "11/11 [==============================] - 166s 15s/step - loss: 1.0638 - accuracy: 0.4638 - precision_4: 0.0000e+00 - val_loss: 1.0074 - val_accuracy: 0.5634 - val_precision_4: 0.0000e+00\n",
      "Epoch 14/15\n",
      "11/11 [==============================] - 183s 17s/step - loss: 1.0627 - accuracy: 0.4638 - precision_4: 0.0000e+00 - val_loss: 1.0077 - val_accuracy: 0.5634 - val_precision_4: 0.0000e+00\n",
      "Epoch 15/15\n",
      "11/11 [==============================] - 162s 15s/step - loss: 1.0667 - accuracy: 0.4638 - precision_4: 0.4096 - val_loss: 1.0007 - val_accuracy: 0.5634 - val_precision_4: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21f9cb5ecd0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x = training_set, validation_data = test_set, epochs = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8f270355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'benign': 0, 'cancer': 1, 'normal': 2}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f2b2a9",
   "metadata": {},
   "source": [
    "## Making one prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6046e800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "test_image = image.load_img('OnePrediction/cancer/5.jpg', target_size = (224, 224),color_mode='grayscale')\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indices\n",
    "\n",
    "the_result = -1\n",
    "value=float('-inf')\n",
    "\n",
    "for i in range(len(result[0])):   \n",
    "    if result[0][i]-value>0.00001: \n",
    "        the_result = i\n",
    "        value = result[0][i]\n",
    "\n",
    "\n",
    "if the_result == 0:\n",
    "    prediction = 'benign'\n",
    "elif the_result:\n",
    "    prediction = 'cancer'\n",
    "else: \n",
    "    prediction = 'normal'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f7440999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cancer\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2425c88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
